{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d9bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1c909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fed6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the full dataset\n",
    "# (note that flights_train is not available in github due to size. only in local repository)\n",
    "chunk = pd.read_csv('../datasets/flights_train_set.csv', \n",
    "#                     usecols = usecols, \n",
    "                    chunksize=1000000, \n",
    "                    low_memory=False)\n",
    "df_full = pd.concat(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304678ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing processing files \n",
    "import sys\n",
    "sys.path.insert(0, '../py_scripts/')\n",
    "from dataset_processing import *\n",
    "from feature_generation_for_multiclass import *\n",
    "from training_and_testing_prep import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e956b7",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12a16d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fl_date</th>\n",
       "      <th>mkt_unique_carrier</th>\n",
       "      <th>branded_code_share</th>\n",
       "      <th>mkt_carrier</th>\n",
       "      <th>mkt_carrier_fl_num</th>\n",
       "      <th>op_unique_carrier</th>\n",
       "      <th>tail_num</th>\n",
       "      <th>op_carrier_fl_num</th>\n",
       "      <th>origin_airport_id</th>\n",
       "      <th>origin</th>\n",
       "      <th>origin_city_name</th>\n",
       "      <th>dest_airport_id</th>\n",
       "      <th>dest</th>\n",
       "      <th>dest_city_name</th>\n",
       "      <th>crs_dep_time</th>\n",
       "      <th>dep_time</th>\n",
       "      <th>dep_delay</th>\n",
       "      <th>taxi_out</th>\n",
       "      <th>wheels_off</th>\n",
       "      <th>wheels_on</th>\n",
       "      <th>taxi_in</th>\n",
       "      <th>crs_arr_time</th>\n",
       "      <th>arr_time</th>\n",
       "      <th>arr_delay</th>\n",
       "      <th>cancelled</th>\n",
       "      <th>cancellation_code</th>\n",
       "      <th>diverted</th>\n",
       "      <th>dup</th>\n",
       "      <th>crs_elapsed_time</th>\n",
       "      <th>actual_elapsed_time</th>\n",
       "      <th>air_time</th>\n",
       "      <th>flights</th>\n",
       "      <th>distance</th>\n",
       "      <th>carrier_delay</th>\n",
       "      <th>weather_delay</th>\n",
       "      <th>nas_delay</th>\n",
       "      <th>security_delay</th>\n",
       "      <th>late_aircraft_delay</th>\n",
       "      <th>first_dep_time</th>\n",
       "      <th>total_add_gtime</th>\n",
       "      <th>longest_add_gtime</th>\n",
       "      <th>no_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-07-14</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>423</td>\n",
       "      <td>NK</td>\n",
       "      <td>N633NK</td>\n",
       "      <td>423</td>\n",
       "      <td>10721</td>\n",
       "      <td>BOS</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>13204</td>\n",
       "      <td>MCO</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>1550</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1552.0</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1910</td>\n",
       "      <td>1828.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>200.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-07-14</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>NK</td>\n",
       "      <td>424</td>\n",
       "      <td>NK</td>\n",
       "      <td>N684NK</td>\n",
       "      <td>424</td>\n",
       "      <td>12892</td>\n",
       "      <td>LAX</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>13487</td>\n",
       "      <td>MSP</td>\n",
       "      <td>Minneapolis, MN</td>\n",
       "      <td>1828</td>\n",
       "      <td>1834.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>2353.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2359</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>211.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     fl_date mkt_unique_carrier branded_code_share mkt_carrier  \\\n",
       "0           0  2018-07-14                 NK                 NK          NK   \n",
       "1           1  2018-07-14                 NK                 NK          NK   \n",
       "\n",
       "   mkt_carrier_fl_num op_unique_carrier tail_num  op_carrier_fl_num  \\\n",
       "0                 423                NK   N633NK                423   \n",
       "1                 424                NK   N684NK                424   \n",
       "\n",
       "   origin_airport_id origin origin_city_name  dest_airport_id dest  \\\n",
       "0              10721    BOS       Boston, MA            13204  MCO   \n",
       "1              12892    LAX  Los Angeles, CA            13487  MSP   \n",
       "\n",
       "    dest_city_name  crs_dep_time  dep_time  dep_delay  taxi_out  wheels_off  \\\n",
       "0      Orlando, FL          1550    1540.0      -10.0      12.0      1552.0   \n",
       "1  Minneapolis, MN          1828    1834.0        6.0      17.0      1851.0   \n",
       "\n",
       "   wheels_on  taxi_in  crs_arr_time  arr_time  arr_delay  cancelled  \\\n",
       "0     1820.0      8.0          1910    1828.0      -42.0          0   \n",
       "1     2353.0      5.0          2359    2358.0       -1.0          0   \n",
       "\n",
       "   cancellation_code  diverted dup  crs_elapsed_time  actual_elapsed_time  \\\n",
       "0                NaN         0   N             200.0                168.0   \n",
       "1                NaN         0   N             211.0                204.0   \n",
       "\n",
       "   air_time  flights  distance  carrier_delay  weather_delay  nas_delay  \\\n",
       "0     148.0        1      1121            NaN            NaN        NaN   \n",
       "1     182.0        1      1535            NaN            NaN        NaN   \n",
       "\n",
       "   security_delay  late_aircraft_delay  first_dep_time  total_add_gtime  \\\n",
       "0             NaN                  NaN             NaN              NaN   \n",
       "1             NaN                  NaN             NaN              NaN   \n",
       "\n",
       "   longest_add_gtime  no_name  \n",
       "0                NaN      NaN  \n",
       "1                NaN      NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f41d4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation (df_full, save_features=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    input: none but df_full = flights_csv full dataset after initial cleaning\n",
    "    and test/train split should already be declared in the notebook.\n",
    "    \n",
    "    generates the aggregate features used for model training\n",
    "    \n",
    "    returns: either returns the dataframes or it saves them to csv\n",
    "    \"\"\"\n",
    "    tmp = preprocessing_dataset(df_full)\n",
    "    \n",
    "\n",
    "    tmp2 = tailnum_delay_taxi_multiclass_params(tmp)\n",
    "    tmp3, tmp4 = tailnum_hourly_delays_multiclass_params(tmp)\n",
    "    tmp5 = carrier_branded_dayofweek_delay_multiclass_params(tmp)\n",
    "    tmp6 = dest_monthly_multiclass_params(tmp)\n",
    "    tmp7 = origin_monthly_multiclass_params(tmp)\n",
    "    tmp8 = holiday_multiclass_params(tmp)\n",
    "    tmp9 = origin_dest_route_dayofweek_multiclass_params(tmp)\n",
    "    \n",
    "    # save to file\n",
    "    if save_features:\n",
    "        tmp2.to_csv('../data/features_tailnum_delay_taxi_multiclass_params.csv')\n",
    "        tmp3.to_csv('../data/tailnum_hourly_delays_multiclass_params_dep.csv')\n",
    "        tmp4.to_csv('../data/tailnum_hourly_delays_multiclass_params_arr.csv')\n",
    "        tmp5.to_csv('../data/carrier_branded_dayofweek_delay_multiclass_params.csv')\n",
    "        tmp6.to_csv('../data/dest_monthly_multiclass_params.csv')\n",
    "        tmp7.to_csv('../data/origin_monthly_multiclass_params.csv')\n",
    "        tmp8.to_csv('../data/holiday_multiclass_params.csv')\n",
    "        tmp9.to_csv('../data/origin_dest_route_dayofweek_multiclass_params.csv')\n",
    "        return tmp\n",
    "    else:       \n",
    "        return tmp, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, tmp8, tmp9\n",
    "        \n",
    "        \n",
    "def preprocessing_dataset(df):\n",
    "    \"\"\"\n",
    "    Input: full dataset or a sample dataset of flights_csv after initial cleaning (check duplicates etc)\n",
    "    returns: clean dataset (no null values) and only records of delayed flights for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # cleaning 'arr_delay' and 'dep_delay'\n",
    "    # remove any null values that are left after calling the cleaning function\n",
    "    df = cleaning_delays(df)\n",
    "    df.dropna(subset=['arr_delay', 'dep_delay'], inplace=True) \n",
    "    unused_cols = ['wheels_off', \n",
    "                    'wheels_on',\n",
    "                    'diverted',\n",
    "                    'cancellation_code',\n",
    "                    'dup',\n",
    "                    'first_dep_time',\n",
    "                    'total_add_gtime',\n",
    "                    'longest_add_gtime',\n",
    "                    'no_name']\n",
    "    df = df.drop(columns=unused_cols) # delete unnecessary cols\n",
    "    # column for the target labels\n",
    "    # clean the delay variables, fill with 0, assuming nan delays were 0\n",
    "    delay_cols = ['carrier_delay', 'weather_delay',\n",
    "       'nas_delay', 'security_delay', 'late_aircraft_delay'] \n",
    "    for col in delay_cols:\n",
    "        df[col].fillna(0, inplace=True) \n",
    "    \n",
    "    \n",
    "    # filter out records where there were no delays\n",
    "    df['isDelay'] = df['arr_delay'].apply(lambda x: 1 if x>0 else None)\n",
    "    df['isDepDelay'] = df['dep_delay'].apply(lambda x: 1 if x>0 else None)\n",
    "    df['isDelay'].fillna(df['isDepDelay'], inplace=True)\n",
    "    df.drop(columns=['isDepDelay'], inplace=True)\n",
    "    df.dropna(subset='isDelay', inplace=True)\n",
    "    \n",
    "    # defining the target (y) labels\n",
    "    df['target_delay'] = df[delay_cols].idxmax(axis=1) # returns maximum delay\n",
    "    \n",
    "\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "def cleaning_delays (df_sample):\n",
    "    \"\"\"input flights csv full dataset or sample data\n",
    "    checks null values for dep_delay and arr_delay \n",
    "    against crs_times and actual times to confirm they are null and not 0s\n",
    "    usually CALLED by preprocessing_dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # checking for Null values \n",
    "    filter1 = df_sample['dep_delay'].isna()\n",
    "    filter2 = (df_sample['crs_dep_time'] == df_sample['dep_time'])\n",
    "\n",
    "    indices = df_sample[(filter1) & (filter2)].index\n",
    "\n",
    "    for idx in indices:\n",
    "        df_sample.loc[idx,'dep_delay'] = 0\n",
    "    \n",
    "    filter1 = df_sample['arr_delay'].isna()\n",
    "    filter2 = (df_sample['crs_arr_time'] == df_sample['arr_time'])\n",
    "\n",
    "    indices = df_sample[(filter1) & (filter2)].index\n",
    "\n",
    "    for idx in indices:\n",
    "        df_sample.loc[idx,'arr_delay'] = 0\n",
    "        \n",
    "        \n",
    "    return df_sample\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "### All scripts should be run on flights dataset (full or sample) after preprocessing_dataset function.\n",
    "\n",
    "def tailnum_delay_taxi_multiclass_params(df_sample):\n",
    "    \"\"\"\n",
    "    Input: flights csv sample or full dataset AFTER preprocessing_dataset\n",
    "    Aggregates on tail_num\n",
    "    Output: \n",
    "        index / join key: 'tail_num'\n",
    "        columns: aggregated isCraft and isCarrier delays \n",
    "    \"\"\"  \n",
    "    \n",
    "    df_sample['isCraft'] = df_sample['target_delay'].\\\n",
    "                            apply (lambda x: 1 if x == 'late_aircraft_delay' else 0)\n",
    "    df_sample['isCarrier'] = df_sample['target_delay'].\\\n",
    "                            apply (lambda x: 1 if x == 'carrier_delay' else 0)\n",
    "\n",
    "    tailnum_delay_taxi_df = df_sample.groupby('tail_num').agg({'dep_delay': 'median',\n",
    "                                  'arr_delay' : 'median',\n",
    "                                  'isCraft' : 'mean',\n",
    "                                  'isCarrier' : 'mean'      \n",
    "                                  })\n",
    "    return tailnum_delay_taxi_df\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def tailnum_hourly_delays_multiclass_params(df_sample):\n",
    "    \"\"\"\n",
    "    Input: flights csv sample or full dataset AFTER preprocessing_dataset\n",
    "    Aggregates on tail_num and arr_hour and tail_num and dep_hour\n",
    "    Output: 2 dataframes \n",
    "        index / join key: 'tail_num' and arr_hour / dep_hour\n",
    "        columns: median delays\n",
    "    \"\"\"  \n",
    "    df_sample['isCraft'] = df_sample['target_delay'].\\\n",
    "                            apply (lambda x: 1 if x == 'late_aircraft_delay' else 0)\n",
    "    df_sample['isCarrier'] = df_sample['target_delay'].\\\n",
    "                            apply (lambda x: 1 if x == 'carrier_delay' else 0)\n",
    "    \n",
    "    # extract hour and minute from crs_time\n",
    "    df_sample['dep_hour'] = (np.round(df_sample['crs_dep_time'],-2)/100).astype(int)\n",
    "    df_sample['arr_hour'] = (np.round(df_sample['crs_arr_time'],-2)/100).astype(int)\n",
    "    \n",
    "    tailnum_dep_hourly_delays_df = df_sample.groupby(['tail_num', 'dep_hour']).agg({'dep_delay': 'median',\n",
    "                                          'carrier_delay' :  'median',\n",
    "                                          'late_aircraft_delay' :  'median',\n",
    "                                          'isCraft' : 'mean', \n",
    "                                          'isCarrier' : 'mean' })\n",
    "    tailnum_arr_hourly_delays_df = df_sample.groupby(['tail_num', 'arr_hour']).agg({'arr_delay': 'median',\n",
    "                                          'carrier_delay' :  'median',\n",
    "                                          'late_aircraft_delay' :  'median',\n",
    "                                          'isCraft' : 'mean', \n",
    "                                          'isCarrier' : 'mean' })\n",
    "    \n",
    "    return tailnum_dep_hourly_delays_df, tailnum_arr_hourly_delays_df\n",
    "    \n",
    "    \n",
    "def carrier_branded_dayofweek_delay_multiclass_params(df_sample):\n",
    "    \"\"\"\n",
    "    Input: flights csv sample or full dataset AFTER preprocessing_dataset\n",
    "    Aggregates on op_unique_carrier, branded_share, f1_dayofweek\n",
    "    \n",
    "    Output: \n",
    "        index /join key: op_unique_carrier, branded_share, f1_dayofweek\n",
    "        columns: median delays and isCarrier\n",
    "    \"\"\"      \n",
    "    \n",
    "\n",
    "    df_sample['branded_share'] = df_sample['branded_code_share'].apply(lambda x: 1 if len(x)>2 else 0)\n",
    "    df_sample = df_sample.drop(columns = ['branded_code_share'])\n",
    "    \n",
    "    \n",
    "    df_sample['isCarrier'] = df_sample['target_delay'].\\\n",
    "                            apply (lambda x: 1 if x == 'carrier_delay' else 0)\n",
    "\n",
    "\n",
    "    df_sample['fl_date'] = pd.to_datetime(df_sample['fl_date'])\n",
    "    df_sample['fl_dayofweek'] = df_sample['fl_date'].dt.dayofweek\n",
    "    df_sample.drop(columns=['fl_date'], inplace=True)\n",
    "\n",
    "\n",
    "    carrier_df = df_sample.groupby(['op_unique_carrier', 'branded_share', 'fl_dayofweek'])\\\n",
    "                                    .agg({'dep_delay': 'median',\n",
    "                                          'arr_delay' : 'median',\n",
    "                                          'carrier_delay' :  'median',\n",
    "                                          'late_aircraft_delay' :  'median', \n",
    "                                          'isCarrier' : 'mean' })\n",
    "    return carrier_df\n",
    "\n",
    "def dest_monthly_multiclass_params(df_sample):\n",
    "    \"\"\"\n",
    "    Input: flights csv sample or full dataset AFTER preprocessing_dataset\n",
    "    Aggregates on dest_airport_id, fl_month\n",
    "    \n",
    "    Output: 2 dataframes \n",
    "        index / join key: dest_airport_id, fl_month\n",
    "        columns: median delays and isWeather, isSecurity\n",
    "    \"\"\"   \n",
    "\n",
    "    # extract hour and minute from crs_time\n",
    "    df_sample['fl_date'] = pd.to_datetime(df_sample['fl_date'])\n",
    "    df_sample['fl_month'] = df_sample['fl_date'].dt.month\n",
    "    \n",
    "    df_sample['isWeather'] = df_sample['target_delay'].\\\n",
    "                            apply (lambda x: 1 if x == 'weather_delay' else 0)\n",
    "    df_sample['isSecurity'] = df_sample['target_delay'].\\\n",
    "                            apply (lambda x: 1 if x == 'security_delay' else 0)\n",
    "\n",
    "    \n",
    "    dest_monthly_params = df_sample.groupby(['dest_airport_id', 'fl_month']).agg({'arr_delay': 'median',\n",
    "                                  'arr_delay' : 'median',\n",
    "                                  'carrier_delay': 'median',  \n",
    "                                  'nas_delay': 'median', \n",
    "                                  'late_aircraft_delay': 'median',                                    \n",
    "                                  'weather_delay' : 'median',\n",
    "                                  'security_delay' : 'median', \n",
    "                                  'isWeather' : 'mean' , \n",
    "                                  'isSecurity' : 'mean' ,\n",
    "                                                                                  \n",
    "                                  })\n",
    "    \n",
    "    dest_monthly_params.index.to_flat_index()\n",
    "    return dest_monthly_params\n",
    "    \n",
    "def origin_monthly_multiclass_params(df_sample):\n",
    "    \"\"\"\n",
    "    Input: flights csv sample or full dataset AFTER preprocessing_dataset\n",
    "    Aggregates on origin_airport_id, fl_month\n",
    "          \n",
    "    Output: A dataframe\n",
    "        index / join key: 'origin_airport_id', 'fl_month'\n",
    "        columns: median delays and isWeather, isSecurity\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # extract hour and minute from crs_time\n",
    "    df_sample['fl_date'] = pd.to_datetime(df_sample['fl_date'])\n",
    "    df_sample['fl_month'] = df_sample['fl_date'].dt.month\n",
    "\n",
    "    df_sample['isWeather'] = df_sample['target_delay'].\\\n",
    "                            apply (lambda x: 1 if x == 'weather_delay' else 0)\n",
    "    df_sample['isSecurity'] = df_sample['target_delay'].\\\n",
    "                            apply (lambda x: 1 if x == 'security_delay' else 0)\n",
    "    \n",
    "    origin_monthly_params = df_sample.groupby(['origin_airport_id', 'fl_month']).agg({'dep_delay': 'median',\n",
    "                                  'arr_delay' : 'median',\n",
    "                                  'carrier_delay': 'median',  \n",
    "                                  'nas_delay': 'median', \n",
    "                                  'late_aircraft_delay': 'median',                                    \n",
    "                                  'weather_delay' : 'median',\n",
    "                                  'security_delay' : 'median', \n",
    "                                  'isWeather' : 'mean', \n",
    "                                  'isSecurity' : 'mean',\n",
    "                                  })\n",
    "    \n",
    "    origin_monthly_params.index.to_flat_index()\n",
    "    return origin_monthly_params\n",
    "    \n",
    "def holiday_multiclass_params(df_sample):\n",
    "    \"\"\"\n",
    "    Input: flights csv sample or full dataset AFTER preprocessing_dataset\n",
    "    Aggregates on 'holidate', 'origin_airport_id', 'dest_airport_id'\n",
    "    Output: \n",
    "        Index / join key: 'holidate', 'origin_airport_id', 'dest_airport_id'\n",
    "        columns: median delays,  isWeather isSecurity\n",
    "\n",
    "    \"\"\"\n",
    "       \n",
    "        \n",
    "########################################\n",
    "##  run this if holidays is not available. check the file location first\n",
    "    us_holidays_df = pd.read_csv('../extra/us_holidays.csv')\n",
    "\n",
    "    from datetime import timedelta\n",
    "    holidays = []\n",
    "    for hol in us_holidays_df['date'].values:\n",
    "        holstart = pd.to_datetime(hol) - timedelta(days=3)\n",
    "        holend = pd.to_datetime(hol) + timedelta(days=3)\n",
    "        holidayweek = pd.date_range(holstart, holend)\n",
    "        holidays.extend(holidayweek)\n",
    "#######################\n",
    "    \n",
    "    df_sample['isWeather'] = df_sample['target_delay'].\\\n",
    "                            apply (lambda x: 1 if x == 'weather_delay' else 0)\n",
    "    df_sample['isSecurity'] = df_sample['target_delay'].\\\n",
    "                            apply (lambda x: 1 if x == 'security_delay' else 0)\n",
    "\n",
    "    \n",
    "    # get holidate \n",
    "    df_sample['fl_date'] = pd.to_datetime(df_sample['fl_date'])\n",
    "    df_sample['holidate'] = df_sample['fl_date'].apply(lambda x: 1 if x in holidays else 0)\n",
    "\n",
    "\n",
    "    holiday_params = df_sample.groupby(['holidate', 'origin_airport_id', 'dest_airport_id']).agg({'dep_delay': 'median',\n",
    "                              'arr_delay' : 'median',\n",
    "                              'carrier_delay': 'median',  \n",
    "                              'nas_delay': 'median', \n",
    "                              'late_aircraft_delay': 'median',                                    \n",
    "                              'weather_delay' : 'median',\n",
    "                              'security_delay' : 'median',\n",
    "                              'isWeather' : 'mean', \n",
    "                              'isSecurity' : 'mean',\n",
    "                              })\n",
    "    \n",
    "    return holiday_params\n",
    "        \n",
    "        \n",
    "def origin_dest_route_dayofweek_multiclass_params(df_sample):\n",
    "    \"\"\"\n",
    "    Input: flights csv sample or full dataset AFTER preprocessing_dataset\n",
    "    Aggregates on 'origin_airport_id', 'dest_airport_id', 'fl_dayofweek'\n",
    "    \n",
    "    Output: A dataframe\n",
    "        Index: 'origin_airport_id', 'dest_airport_id', 'fl_dayofweek'\n",
    "        columns: median delays, isCarrier\n",
    "    \"\"\"\n",
    "    \n",
    "    df_sample['isCarrier'] = df_sample['target_delay'].\\\n",
    "                            apply (lambda x: 1 if x == 'carrier_delay' else 0)\n",
    "    \n",
    "    # get dayofweek\n",
    "    df_sample['fl_date'] = pd.to_datetime(df_sample['fl_date'])\n",
    "    df_sample['fl_dayofweek'] = df_sample['fl_date'].dt.dayofweek\n",
    "    # traffic\n",
    "    \n",
    "    params_df = df_sample.groupby(['origin_airport_id', 'dest_airport_id', \n",
    "                                   'fl_dayofweek']).agg({'dep_delay': 'median',\n",
    "                              'arr_delay' : 'median',\n",
    "                              'carrier_delay': 'median',  \n",
    "                              'nas_delay': 'median', \n",
    "                              'late_aircraft_delay': 'median',                                    \n",
    "                              'weather_delay' : 'median',\n",
    "                              'security_delay' : 'median',\n",
    "                              'isCarrier' : 'mean'\n",
    "                              })\n",
    "\n",
    "    params_df['traffic'] = df_sample.groupby(['origin_airport_id', 'dest_airport_id', 'fl_dayofweek']).size()\n",
    "    \n",
    "    \n",
    "    return params_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bea2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_training_df(train_df):\n",
    "    \"\"\"Input: dataset after preprocessing\n",
    "    check extras for  holidays csv\n",
    "    Output: 2 datasets: X and y\n",
    "    \"\"\"\n",
    "\n",
    "    ######################################################\n",
    "    # getting the list of US national holidays\n",
    "    # run this if 'holidays' is not available. check the file location first\n",
    "    us_holidays_df = pd.read_csv('../extra/us_holidays.csv')\n",
    "\n",
    "    from datetime import timedelta\n",
    "    holidays = []\n",
    "    for hol in us_holidays_df['date'].values:\n",
    "        holstart = pd.to_datetime(hol) - timedelta(days=5)\n",
    "        holend = pd.to_datetime(hol) + timedelta(days=3)\n",
    "        holidayweek = pd.date_range(holstart, holend)\n",
    "        holidays.extend(holidayweek)\n",
    "    #######################################################          \n",
    "\n",
    "\n",
    "    # EDA: column transformations to integrate with the preprocessed feature tables:\n",
    "    # binarize branded share\n",
    "    train_df['branded_share'] = train_df['branded_code_share'].apply(lambda x: 1 if len(x)>2 else 0)\n",
    "    # extract month and day of week and holidate\n",
    "    train_df['fl_date'] = pd.to_datetime(train_df['fl_date'])\n",
    "    train_df['fl_month'] = train_df['fl_date'].dt.month\n",
    "    train_df['fl_dayofweek'] = train_df['fl_date'].dt.dayofweek\n",
    "    train_df['fl_date'] = pd.to_datetime(train_df['fl_date'])\n",
    "    train_df['holidate'] = train_df['fl_date'].apply(lambda x: 1 if x in holidays else 0)\n",
    "\n",
    "    # extract flight hour\n",
    "    train_df['dep_hour'] = (np.round(train_df['crs_dep_time'],-2)/100).astype(int)\n",
    "    train_df['arr_hour'] = (np.round(train_df['crs_arr_time'],-2)/100).astype(int)\n",
    "\n",
    "    # drop irrelevant columns  \n",
    "    train_df.drop(columns = ['mkt_unique_carrier', 'mkt_carrier',\n",
    "                         'mkt_carrier_fl_num',\n",
    "                         'op_carrier_fl_num',\n",
    "                         'origin', 'origin_city_name'],\n",
    "                         inplace=True)\n",
    "\n",
    "    train_df.drop(columns = ['dest', 'dest_city_name',\n",
    "                         'crs_elapsed_time',\n",
    "                         'flights',\n",
    "                         'fl_date',\n",
    "                         'crs_dep_time', 'crs_arr_time',\n",
    "                         'branded_code_share'],\n",
    "                          inplace=True)\n",
    "\n",
    "    delay_cols = ['carrier_delay', 'weather_delay',\n",
    "                   'nas_delay', 'security_delay', 'late_aircraft_delay'] \n",
    "\n",
    "#     # defining the target (y) labels\n",
    "#     df['target'] = df[delay_cols].idxmax(axis=1) # returns maximum delay\n",
    "\n",
    "    # remove delays from dataset\n",
    "    train_df.drop(columns=delay_cols, inplace=True)\n",
    "\n",
    "    train_df.drop(columns = ['dep_time',\n",
    "                       'dep_delay', 'taxi_out', 'taxi_in', 'arr_time',\n",
    "                       'arr_delay', 'cancelled','actual_elapsed_time',\n",
    "                       'air_time', 'isDelay'],\n",
    "                              inplace=True)\n",
    " \n",
    "    ################# calling features tables\n",
    "    \n",
    "    # merging the testing dataset with the features tables of aggregate values\n",
    "    # thereby converting categorical and ordinal columns to continuous values\n",
    "    tmp = train_df\n",
    "    train_df = tmp.merge(features_2, \n",
    "                  left_on=['tail_num'], \n",
    "                  right_on=['tail_num'], how='left').merge(features_3,\n",
    "                  left_on=['tail_num','dep_hour'],\n",
    "                  right_on=['tail_num','dep_hour']).merge(features_4,\n",
    "                  left_on=['tail_num','arr_hour'],\n",
    "                  right_on=['tail_num','arr_hour']).merge(features_5,\n",
    "                  left_on=['op_unique_carrier', 'branded_share', 'fl_dayofweek'], \n",
    "                  right_on=['op_unique_carrier', 'branded_share', 'fl_dayofweek'],\n",
    "                  suffixes=('_', '_carrier')).merge(features_6,\n",
    "                  left_on=['dest_airport_id', 'fl_month'], \n",
    "                  right_on=['dest_airport_id', 'fl_month'],\n",
    "                  suffixes=('_', '_dest')).merge(features_7,\n",
    "                  left_on=['origin_airport_id', 'fl_month'], \n",
    "                  right_on=['origin_airport_id', 'fl_month'],\n",
    "                  suffixes=('_', '_origin')).merge(features_8,                                \n",
    "                  left_on=['holidate', 'origin_airport_id', 'dest_airport_id'], \n",
    "                  right_on=['holidate', 'origin_airport_id', 'dest_airport_id'],\n",
    "                  suffixes=('_', '_holidate')).merge(features_9,                               \n",
    "                  left_on=['origin_airport_id', 'dest_airport_id', 'fl_dayofweek'], \n",
    "                  right_on=['origin_airport_id', 'dest_airport_id', 'fl_dayofweek'],\n",
    "                  suffixes=('_', '_route'))\n",
    "    \n",
    "    # dropping irrelevant columns\n",
    "    train_y = train_df['target_delay']\n",
    "    \n",
    "    train_X = train_df.drop(columns = ['op_unique_carrier',\n",
    "                       'tail_num',\n",
    "                       'origin_airport_id',\n",
    "                       'dest_airport_id', 'target_delay',\n",
    "                                       'fl_month',\n",
    "                                      'fl_dayofweek'])\n",
    "    \n",
    "    return train_y, train_X\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preparing_test_dataset(df):\n",
    "    \"\"\"Input: 'raw' testing dataframe from csv file\n",
    "    This function takes the raw pd.read_csv('flights_test.csv') \n",
    "        applies the engineered feature aggregations\n",
    "        and restructures it to the form it needs to be \n",
    "        for Scaling and Model Predicting\n",
    "    Output: X_test. \n",
    "    \"\"\"\n",
    "    \n",
    "    ######################################################\n",
    "    # getting the list of US national holidays\n",
    "    # run this if 'holidays' is not available. check the file location first\n",
    "    us_holidays_df = pd.read_csv('extra/us_holidays.csv')\n",
    "\n",
    "    from datetime import timedelta\n",
    "    holidays = []\n",
    "    for hol in us_holidays_df['date'].values:\n",
    "        holstart = pd.to_datetime(hol) - timedelta(days=5)\n",
    "        holend = pd.to_datetime(hol) + timedelta(days=3)\n",
    "        holidayweek = pd.date_range(holstart, holend)\n",
    "        holidays.extend(holidayweek)\n",
    "    #######################################################   \n",
    "    \n",
    "    # column transformations to integrate with the preprocessed feature tables:  \n",
    "\n",
    "    # binarize branded share\n",
    "    df['branded_share'] = df['branded_code_share'].apply(lambda x: 1 if len(x)>2 else 0)\n",
    "    # extract month and day of week and holidate\n",
    "    df['fl_date'] = pd.to_datetime(df['fl_date'])\n",
    "    df['fl_month'] = df['fl_date'].dt.month\n",
    "    df['fl_dayofweek'] = df['fl_date'].dt.dayofweek\n",
    "    df['fl_date'] = pd.to_datetime(df['fl_date'])\n",
    "    df['holidate'] = df['fl_date'].apply(lambda x: 1 if x in holidays else 0)\n",
    "     # extract flight hour\n",
    "    df['dep_hour'] = (np.round(df['crs_dep_time'],-2)/100).astype(int)\n",
    "    df['arr_hour'] = (np.round(df['crs_arr_time'],-2)/100).astype(int)   \n",
    "    # drop irrelevant columns   \n",
    "    df.drop(columns = ['dup', 'mkt_unique_carrier', 'mkt_carrier',\n",
    "                         'mkt_carrier_fl_num',\n",
    "                         'op_carrier_fl_num',\n",
    "                         'origin', 'origin_city_name',\n",
    "                         'dest', 'dest_city_name',\n",
    "                         'crs_elapsed_time',\n",
    "                         'flights',\n",
    "                            'fl_date',\n",
    "                            'crs_dep_time', 'crs_arr_time',\n",
    "                         'branded_code_share'],\n",
    "              inplace=True) \n",
    "    \n",
    "    ################# calling features tables\n",
    "    \n",
    "    # merging the testing dataset with the features tables of aggregate values\n",
    "    # thereby converting categorical and ordinal columns to continuous values\n",
    "    tmp = df\n",
    "    df = tmp.merge(features_features_2, \n",
    "                  left_on=['tail_num'], \n",
    "                  right_on=['tail_num'], how='left').merge(features_features_3,\n",
    "                  left_on=['tail_num','dep_hour'],\n",
    "                  right_on=['tail_num','dep_hour']).merge(features_features_4,\n",
    "                  left_on=['tail_num','arr_hour'],\n",
    "                  right_on=['tail_num','arr_hour']).merge(features_features_5,\n",
    "                  left_on=['op_unique_carrier', 'branded_share', 'fl_dayofweek'], \n",
    "                  right_on=['op_unique_carrier', 'branded_share', 'fl_dayofweek'],\n",
    "                  suffixes=('_', '_carrier')).merge(features_features_6,\n",
    "                  left_on=['dest_airport_id', 'fl_month'], \n",
    "                  right_on=['dest_airport_id', 'fl_month'],\n",
    "                  suffixes=('_', '_dest')).merge(features_features_7,\n",
    "                  left_on=['origin_airport_id', 'fl_month'], \n",
    "                  right_on=['origin_airport_id', 'fl_month'],\n",
    "                  suffixes=('_', '_origin')).merge(features_features_8,                                \n",
    "                  left_on=['holidate', 'origin_airport_id', 'dest_airport_id'], \n",
    "                  right_on=['holidate', 'origin_airport_id', 'dest_airport_id'],\n",
    "                  suffixes=('_', '_holidate')).merge(features_features_9,                               \n",
    "                  left_on=['origin_airport_id', 'dest_airport_id', 'fl_dayofweek'], \n",
    "                  right_on=['origin_airport_id', 'dest_airport_id', 'fl_dayofweek'],\n",
    "                  suffixes=('_', '_route'))\n",
    "    \n",
    "    df.drop(columns = ['op_unique_carrier',\n",
    "                       'tail_num',\n",
    "                       'origin_airport_id',\n",
    "                       'dest_airport_id',\n",
    "                                       'fl_month',\n",
    "                                      'fl_dayofweek'],\n",
    "              inplace=True)\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59bf9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2 = pd.read_csv('../data/features_tailnum_delay_taxi_multiclass_params.csv')\n",
    "features_3 = pd.read_csv('../data/tailnum_hourly_delays_multiclass_params_dep.csv')\n",
    "features_4 = pd.read_csv('../data/tailnum_hourly_delays_multiclass_params_arr.csv')\n",
    "features_5 = pd.read_csv('../data/carrier_branded_dayofweek_delay_multiclass_params.csv')\n",
    "features_6 = pd.read_csv('../data/dest_monthly_multiclass_params.csv')\n",
    "features_7 = pd.read_csv('../data/origin_monthly_multiclass_params.csv')\n",
    "features_8 = pd.read_csv('../data/holiday_multiclass_params.csv')\n",
    "features_9 = pd.read_csv('../data/origin_dest_route_dayofweek_multiclass_params.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf709f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b25db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07017e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85feb41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126d3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b2b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9cddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2854ba4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e8cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f8ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c16e3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e713d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de9ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd2ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a050f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eaa053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277e0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773031d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24264a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75e0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a3a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205dc81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c734c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b205cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8cbe32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67117fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a849f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ef92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6fd102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535cf9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "df = feature_generation(df_full, 1)\n",
    "# this function calls the necessary functions to clean the dataset \n",
    "# and generate the aggregate features for model training\n",
    "# save_features = True saves the files into the local directory, save_features False returns them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa4854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7056e5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69c173ec",
   "metadata": {},
   "source": [
    "## Step 1: Testing on a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f30e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload full dataset\n",
    "chunk = pd.read_csv('../datasets/flights_train_set.csv', \n",
    "#                     usecols = usecols, \n",
    "                    chunksize=1000000, \n",
    "                    low_memory=False)\n",
    "df_full = pd.concat(chunk)\n",
    "df_full.drop(columns = 'Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327e8c7",
   "metadata": {},
   "source": [
    "### Preparing the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e25dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample\n",
    "df_sample = df_full.sample(n=3000000)\n",
    "\n",
    "# getting the target and value datasets\n",
    "tmp_train_batch = preprocessing_dataset(df_sample)\n",
    "y_batch, X_batch = preparing_training_df(tmp_train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "60a0955f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1323842,), (1323842, 59))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.shape, X_batch.shape # check shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "36f804e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carrier_delay          901069\n",
       "late_aircraft_delay    226213\n",
       "nas_delay              176164\n",
       "weather_delay           19408\n",
       "security_delay            988\n",
       "Name: target_delay, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98404c",
   "metadata": {},
   "source": [
    "Due to the unbalanced classes, we have to strategically create sample data for training.\n",
    "\n",
    "We will try 2 strategies: \n",
    "\n",
    "1. underbalancing the bigger classes or \n",
    "\n",
    "2. overbalancing the smaller classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8029ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating filters for the target classes\n",
    "Y_carrier = y_batch == 'carrier_delay'\n",
    "Y_aircraft = y_batch == 'late_aircraft_delay'\n",
    "Y_nas = y_batch == 'nas_delay'\n",
    "Y_weather = y_batch == 'weather_delay'\n",
    "Y_security = y_batch == 'security_delay'\n",
    "\n",
    "# to get the indices of the records\n",
    "y_carrier_index = y_batch[Y_carrier].index\n",
    "y_security_index = y_batch[Y_security].index\n",
    "y_nas_index = y_batch[Y_nas].index\n",
    "y_weather_index = y_batch[Y_weather].index\n",
    "y_aircraft_index = y_batch[Y_aircraft].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b673f",
   "metadata": {},
   "source": [
    "#### 1. underbalancing the bigger classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "524ea8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize the sample size of the classes so they're equal\n",
    "y_security_training_index = y_security_index\n",
    "y_nas_training_index = np.random.choice(y_nas_index, size = 324)\n",
    "y_weather_training_index = np.random.choice(y_weather_index, size = 324)\n",
    "y_carrier_training_index = np.random.choice(y_carrier_index, size = 324)\n",
    "y_air_training_index = np.random.choice(y_aircraft_index, size = 324)\n",
    "\n",
    "\n",
    "# add them together and shuffle\n",
    "shuffle_index = np.concatenate((y_security_training_index,\n",
    "                  y_nas_training_index,\n",
    "                  y_weather_training_index,\n",
    "                  y_carrier_training_index,\n",
    "                  y_air_training_index))\n",
    "np.random.shuffle(shuffle_index)\n",
    "np.random.shuffle(shuffle_index) # to be doubly sure\n",
    "\n",
    "# make X, the parameters based on the shuffle_index\n",
    "\n",
    "X_train = X_batch.iloc[shuffle_index]\n",
    "y_train = y_batch.iloc[shuffle_index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f7303b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "security_delay         988\n",
       "nas_delay              324\n",
       "carrier_delay          324\n",
       "weather_delay          324\n",
       "late_aircraft_delay    324\n",
       "Name: target_delay, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d7e054",
   "metadata": {},
   "source": [
    "**Note**: That a separate test_dataset was reserved before feature generation to avoid data leak. So we will not be using train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df7b1f3",
   "metadata": {},
   "source": [
    "### Preparing the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1a379ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data with train test split\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.20, random_state=88)\n",
    "\n",
    "test_full = preprocessing_dataset(pd.read_csv('../datasets/flights_test_set.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f735acb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4645141, 59), (620613, 59), (4645141,), (620613,))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = test_full\n",
    "y_test, X_test = preparing_training_df(test_batch)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3e44ddf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carrier_delay          422016\n",
       "late_aircraft_delay    106405\n",
       "nas_delay               82603\n",
       "weather_delay            9091\n",
       "security_delay            498\n",
       "Name: target_delay, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts() # checking that it has all the labels we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ac724",
   "metadata": {},
   "source": [
    "* Now that the datasets are ready, model training and testing starts here...\n",
    "* Our first model is Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d1533378",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      9\u001b[0m clf \u001b[38;5;241m=\u001b[39m LogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m88\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# testing the test data\u001b[39;00m\n\u001b[1;32m     13\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1291\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1289\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1291\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/joblib/parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/joblib/parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/joblib/parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:450\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    446\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C\n\u001b[1;32m    447\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[1;32m    448\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[1;32m    449\u001b[0m ]\n\u001b[0;32m--> 450\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[1;32m    459\u001b[0m     solver,\n\u001b[1;32m    460\u001b[0m     opt_res,\n\u001b[1;32m    461\u001b[0m     max_iter,\n\u001b[1;32m    462\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[1;32m    463\u001b[0m )\n\u001b[1;32m    464\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/scipy/optimize/_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    693\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    694\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 696\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    699\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    700\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/scipy/optimize/_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    353\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/scipy/optimize/_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/scipy/optimize/_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 70\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/sklearn/linear_model/_linear_loss.py:297\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    295\u001b[0m grad[:, :n_features] \u001b[38;5;241m=\u001b[39m grad_pointwise\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m X \u001b[38;5;241m+\u001b[39m l2_reg_strength \u001b[38;5;241m*\u001b[39m weights\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept:\n\u001b[0;32m--> 297\u001b[0m     grad[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_pointwise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m coef\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    299\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mravel(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/lhl_env/lib/python3.8/site-packages/numpy/core/_methods.py:48\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# scale the data\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_scaled = scaler.transform(X_train)\n",
    "\n",
    "# train model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=88, max_iter=1500)\n",
    "clf.fit(X_scaled, y_train)\n",
    "\n",
    "# testing the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "clf.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d3dc3",
   "metadata": {},
   "source": [
    "#### 2. overbalancing the smaller classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d3c28777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carrier_delay          901069\n",
       "late_aircraft_delay    226213\n",
       "nas_delay              176164\n",
       "weather_delay           19408\n",
       "security_delay            988\n",
       "Name: target_delay, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "59a94580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize the sample size of the classes so they're equal\n",
    "y_carrier_training_index = y_carrier_index\n",
    "# y_nas_training_index = np.random.choice(y_nas_index, size = 102820)\n",
    "# y_weather_training_index = np.random.choice(y_weather_index, size = 102820)\n",
    "# y_carrier_training_index = np.random.choice(y_carrier_index, size = 57791)\n",
    "# y_air_training_index = np.random.choice(y_aircraft_index, size = 57791)\n",
    "# y_nas_training_index = y_nas_index\n",
    "\n",
    "tmp = []\n",
    "for i in range (4):\n",
    "    tmp.extend(y_aircraft_index)\n",
    "y_air_training_index = tmp\n",
    "\n",
    "tmp = []\n",
    "for i in range (5):\n",
    "    tmp.extend(y_nas_index)\n",
    "y_nas_training_index = tmp\n",
    "\n",
    "\n",
    "tmp = []\n",
    "for i in range (50):\n",
    "    tmp.extend(y_weather_index)\n",
    "y_weather_training_index = tmp\n",
    "\n",
    "tmp = []\n",
    "for i in range (1000):\n",
    "    tmp.extend(y_security_index)\n",
    "y_security_training_index = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "833aa39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2ffa5061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "security_delay         988000\n",
       "weather_delay          970400\n",
       "late_aircraft_delay    904852\n",
       "carrier_delay          901069\n",
       "nas_delay              880820\n",
       "Name: target_delay, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add them together and shuffle\n",
    "shuffle_index = np.concatenate((y_security_training_index,\n",
    "                  y_nas_training_index,\n",
    "                  y_weather_training_index,\n",
    "                  y_carrier_training_index,\n",
    "                  y_air_training_index))\n",
    "np.random.shuffle(shuffle_index)\n",
    "np.random.shuffle(shuffle_index)\n",
    "# make X, the parameters based on the shuffle_index\n",
    "\n",
    "X_train = X_batch.iloc[shuffle_index]\n",
    "y_train = y_batch.iloc[shuffle_index] \n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c46b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and train model\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_scaled = scaler.transform(X_train)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=88, max_iter=1000)\n",
    "clf.fit(X_scaled, y_train)\n",
    "\n",
    "# testing the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "clf.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5018f1f6",
   "metadata": {},
   "source": [
    "## Step 2: Testing on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e6f929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
